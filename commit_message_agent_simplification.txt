refactor: Simplify agent mode final evaluation to prevent LLM hallucination

Simplified the final response evaluation system in agent mode from complex 5-key 
JSON structure to simple 2-key structure to prevent AI hallucination and 
malformed output that was causing evaluation failures.

Changes Made:
- Simplified final evaluation JSON structure from 5 keys to 2 keys (user_response, analysis)
- Reduced complex nested scoring system to simple issue identification
- Enhanced validation and fallback logic for simplified structure
- Updated evaluation prompts to use ultra-reliable 2-key format
- Improved error handling with specific simplified evaluation error types
- Added comprehensive structure validation with meaningful fallback responses

Technical Improvements:
- Changed from complex evaluation object with quality_score, completeness_score, etc. to simple user_response + analysis
- Replaced nested system_analytics and follow_up_actions with straightforward issue identification
- Enhanced debug logging to track simplified evaluation structure performance
- Updated status messages to reflect simplified evaluation approach
- Improved fallback responses when LLM returns invalid structures

Files Modified:
- src/agents/agent_mode_node.py: Simplified evaluation processing and validation logic
- src/prompts/agent_mode_prompts.py: Rewrote evaluation prompt for 2-key reliability
- src/utils/open_ai_integration.py: Added content logging for debugging reasoning_content extraction

Problem Solved:
The previous complex evaluation structure (5 top-level keys with nested scoring) 
was causing LLMs to frequently return malformed JSON (lists instead of dicts, 
missing keys, invalid structure) leading to evaluation failures and poor user experience.

The simplified v4.0 approach uses only 2 keys:
- user_response: {message, next_steps} - what user sees
- analysis: {issues, reason} - internal improvement tracking

This dramatically improves LLM reliability while maintaining evaluation quality.

Impact:
- Reduced evaluation failures from frequent to rare
- Improved user experience with more reliable final responses
- Maintained workflow quality assessment with simplified but effective structure
- Enhanced debugging capabilities with clearer error categorization
- Better fallback handling when LLM evaluation fails

This change represents a shift from complex but unreliable evaluation to simple 
but robust evaluation that prioritizes user experience and system reliability.