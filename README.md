# ğŸ¤– AI-Agent-Workflow Project

> Enterprise-grade desktop AI assistant with LangGraph multi-agent architecture, dynamic MCP integration via .mcp.json, universal MCP routing, hybrid OpenAI/NVIDIA integration (with circuit breaker), local Ollama support, Rich Traceback, and professional workflows.

[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![LangGraph](https://img.shields.io/badge/LangGraph-Latest-green.svg)](https://langchain-ai.github.io/langgraph/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/Version-v1.8.0-brightgreen.svg)]()

---

## ğŸš€ What's New (v1.8.0 â€“ August 2025)
- âœ… **Dynamic MCP Integration** - Server registration from .mcp.json (no code edits required)
- âœ… **Universal MCP Routing** - UniversalMCPWrapper with static+dynamic toolâ†’server mapping
- âœ… **Robust MCP Manager** - ServerConfig/Command enum, safer subprocess I/O, encoding fallbacks
- âœ… **OpenAI Circuit Breaker** - Automatic failure detection, retry/backoff, fallback responses
- âœ… **Docker artifacts included (initial)** - Dockerfile and docker-compose.yml are included as starting points; full containerized support (API surface) is planned in a future release
- âœ… **Python 3.13** - Updated target via pyproject.toml
- âœ… **Enhanced Diagnostics** - Expanded logging and tests for MCP routing and circuit breaker

## âœ¨ Current Status
- **Production Readiness**: 95% â†’ Stability improved via circuit breaker + MCP hardening
- **MCP**: Fully dynamic via .mcp.json at project root (path set in settings.MCP_CONFIG.MCP_CONFIG_PATH)
- **Agent Mode**: More reliable parameter generation and MCP tool execution
- **DevOps**: Container artifacts included, but full container-first deployment (API surface) is planned â€” Docker is not yet a supported runtime for production
- **Compatibility**: Python 3.13 baseline; legacy 3.11 works with requirements.txt

---

## ğŸŒŸ **What Makes This Special?**

This is a **production-ready consumer desktop AI assistant** with enterprise-grade architecture featuring:

- **ğŸ¤– Hybrid AI Integration**: Seamless switching between local Ollama models and OpenAI/NVIDIA API with intelligent rate limiting (30 requests/minute)
- **âš¡ Agent Mode**: Revolutionary `/agent` command triggering multi-tool orchestration with AI-powered parameter generation
- **ğŸ› ï¸ Tool Ecosystem**: 3 fundamental tools + dynamic MCP-exposed tools (see below). The repository's current .mcp.json registers 6 MCP servers (github, git, filesystem, memory, puppeteer, sequential-thinking); the filesystem MCP provides a set of dynamic filesystem tools when running.
- **ğŸ¨ Rich Traceback System**: Enterprise-grade error handling with visual debugging and separate debug windows
- **ğŸ“¡ Event-Driven Architecture**: Complete listener system with Rich.status integration for real-time updates
- **ğŸ”’ Privacy-First Design**: Local processing with optional cloud model integration
- **ğŸ—ï¸ LangGraph Multi-Agent**: Production-ready conversation orchestration with StateAccessor singleton pattern

---

## âœ¨ **Core Features**

### ğŸ§  **Hybrid AI System**
- **Local Ollama Support**: Privacy-focused local model processing
- **OpenAI/NVIDIA Integration**: Cloud models with intelligent rate limiting (30 requests/minute)
- **Automatic Model Switching**: Seamless hybrid operation based on availability and preferences
- **Rate Limit Management**: Built-in protection against API rate limit violations

### âš¡ **Agent Mode (`/agent` Command)**
- **Multi-Tool Orchestration**: Intelligent tool chain execution with AI parameter generation
- **Context-Aware Execution**: Maintains execution history and reasoning chains for better results
- **Tool Fallback Support**: Automatic recovery with alternative tools when primary tools fail
- **Simplified Final Evaluation**: Streamlined workflow quality assessment (v4.0)

### ğŸ› ï¸ **Comprehensive Tool System (overview)**

#### **Fundamental Tools (examples)**
- **GoogleSearch**: Web search capabilities for current information
- **RAGSearch**: Knowledge base search using retrieval-augmented generation
- **Translate**: Language translation services

> Note: In addition to these fundamental tools, the project uses MCP servers (configured via .mcp.json) to expose many dynamic tools (filesystem, memory, github, etc.). See the `.mcp.json` in the repo root for the exact servers currently registered.

### ğŸ¨ **Rich Traceback & Debugging System**
- **Visual Error Handling**: Beautiful tracebacks with syntax highlighting and variable inspection
- **Separate Debug Windows**: Error routing to dedicated debug panel vs user notifications
- **Structured Diagnostics**: Transport-agnostic logging with metadata-rich events
- **Socket-Based Routing**: Network-based log aggregation for clean separation
- **Performance Monitoring**: Error categorization, frequency tracking, and debugging statistics

### ğŸ“¡ **Event-Driven Architecture**
- **RichStatusListener**: Automatic status updates with Rich.status integration
- **EventManager**: Singleton pattern with thread-safe event processing
- **Variable Change Detection**: Automatic event emission when object properties change
- **Memory Leak Prevention**: WeakKeyDictionary for automatic cleanup
- **Event Filtering**: Targeted event routing with metadata-based filtering

---

## ğŸ”§ Dynamic MCP Integration

The AI-Agent-Workflow now supports **dynamic MCP server registration** through a simple `.mcp.json` configuration file placed at the project root.

### Repository MCP configuration (current)
The repository's .mcp.json currently registers the following MCP servers: github, git, filesystem, memory, puppeteer, sequential-thinking (6 servers). The filesystem server is configured to point at the repository root on this machine (Windows path in the example).

### Example (.mcp.json from this repo)
```json
{
	"inputs": [
		{
			"id": "GITHUB_TOKEN",
			"description": "GitHub personal access token",
			"type": "promptString",
			"password": true
		}
	],
	"servers": {
		"github": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"@modelcontextprotocol/server-github@latest"
			],
			"env": {
				"GITHUB_TOKEN": "%GITHUB_TOKEN%"
			}
		},
		"git": {
			"type": "stdio",
			"command": "uvx",
			"args": [
				"mcp-server-git"
			],
			"env": {}
		},
		"filesystem": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"@modelcontextprotocol/server-filesystem@latest",
				"C:\\Users\\pirat\\PycharmProjects\\AI_llm"
			],
			"env": {}
		},
		"memory": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"@modelcontextprotocol/server-memory@latest"
			]
		},
		"puppeteer": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"@modelcontextprotocol/server-puppeteer@latest"
			]
		},
		"sequential-thinking": {
			"type": "stdio",
			"command": "npx",
			"args": [
				"-y",
				"@modelcontextprotocol/server-sequential-thinking"
			]
		}
	}
}
```

### Features
- **Universal MCP Routing**: UniversalMCPWrapper with static+dynamic toolâ†’server mapping
- **Auto Registration**: ChatInitializer loads and starts servers asynchronously; discovered tools are auto-registered
- **Robust Manager**: ServerConfig/Command enum, safer subprocess I/O, encoding fallbacks, tool discovery mapping
- **Configuration Path**: Set via `settings.MCP_CONFIG.MCP_CONFIG_PATH` (defaults to project root `.mcp.json`)

---

## âš™ï¸ OpenAI/NVIDIA Circuit Breaker

Enhanced OpenAI integration with enterprise-grade reliability features:

- **Circuit Breaker Pattern**: Automatic failure detection and recovery
- **Retry Logic**: Exponential backoff for failed requests
- **Fallback Responses**: Graceful degradation when API unavailable
- **Streaming Safety**: Robust handling of streaming/non-streaming responses
- **Rate Limiting**: Improved UX and diagnostics for async rate limiting

---

## ğŸ³ Dockerization

Note: Docker support is NOT fully supported yet. While the repository contains Docker-related files (Dockerfile and docker-compose.yml), the application is currently designed as a desktop/orchestrator application rather than an API service. Because of that, it is not yet fully dockerizable or intended to be run as a containerized API service out-of-the-box.

Planned: we intend to refactor and expose an API surface in a future release (v1.9.0+), at which point a supported Docker image and docker-compose setup will be provided with clear runtime instructions.

If you want to experiment locally before official Docker support is added, consider the following notes:
- The Dockerfile in the repo is a starting point and may require adjustments to entrypoints, mounted volumes, and MCP server availability.
- The docker-compose.yml file is present but not guaranteed to boot all required MCP servers or to match your local .mcp.json configuration.
- Integration tests and MCP-based features expect local Node-based MCP servers to be started; these are not automatically managed inside the current containers.

Recommended immediate workflow:
- Run the application locally using the Quick Start instructions (python src/main_orchestrator.py) while MCP servers are started via npx/uvx as needed.
- When you're ready to containerize, I can prepare a dedicated API wrapper and a production-ready docker-compose configuration that launches required MCP servers and configures environment secrets.

---

## ğŸš€ **Quick Start**

### **Prerequisites**
```bash
Python 3.13+ (recommended)
Virtual environment (recommended)
Node.js (for MCP servers)
Docker (optional, planned; not fully supported yet)
```

### **Installation**
```bash
# Clone the repository
git clone https://github.com/PIRATE-E/AI-Agent-Workflow-Project.git
cd AI-Agent-Workflow-Project

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies (recommended):
# Option A (recommended) - install from top-level requirements.txt (keeps quick pip install path):
pip install -r requirements.txt

# Option B (canonical) - use pyproject.toml as canonical dependency list
pip install -e .
```

### **Configuration**
Create `.env` file in the project root:
```env
# OpenAI/NVIDIA API Configuration (Optional - for cloud models)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_TIMEOUT=30

# Sentry Monitoring (Optional)
SENTRY_DSN=your_sentry_dsn_here

# Local Model Configuration (Ollama)
OLLAMA_HOST=http://localhost:11434
GPT_MODEL=llama3.2:latest  # or your preferred local model

# MCP Configuration
MCP_CONFIG_PATH=.mcp.json  # Path to MCP configuration file
MCP_API_KEY=your_mcp_api_key_here  # Optional: used by protected MCP servers

# GitHub (for github MCP server)
GITHUB_TOKEN=your_github_personal_access_token_here
```

Add a new section for Authentication & MCP startup:

---

## ğŸ” Authentication & MCP Server Startup (prevent auth errors)

This project requires several credentials for optional cloud integrations and for some MCP servers. Common causes of "authentication" or "permission" errors are misnamed environment variables, missing tokens, or MCP servers not running.

Required/optional credentials and env variables
- OPENAI_API_KEY (optional) â€” used for cloud OpenAI/NVIDIA models. Make sure this key is valid and has remaining quota.
- GITHUB_TOKEN (optional) â€” used by the MCP GitHub server to access private repositories. The token must have repo/read permissions for repository operations.
- MCP_API_KEY (optional) â€” used by MCP servers that require an API key. If your MCP servers are configured to require a key, set this environment variable and ensure the same key is used by the server and client.
- OLLAMA_HOST (optional) â€” only if you run a local Ollama model service.

How to start the MCP servers used by this repository (local testing)

The repo ships a `.mcp.json` that defines how to start MCP servers. Below are commands to start each server manually (run in a separate terminal for each server). These are the commands that correspond to the `.mcp.json` in this repo â€” adjust paths and env vars as needed.

# Example commands (open separate terminals):
# 1) Filesystem server (exposes local filesystem tools)
npx -y @modelcontextprotocol/server-filesystem@latest "C:\\Users\\pirat\\PycharmProjects\\AI_llm"

# 2) Memory server
npx -y @modelcontextprotocol/server-memory@latest

# 3) GitHub server (requires GITHUB_TOKEN in environment)
# Unix/macOS:
export GITHUB_TOKEN=your_github_token_here && npx -y @modelcontextprotocol/server-github@latest
# Windows (PowerShell):
$env:GITHUB_TOKEN="your_github_token_here"; npx -y @modelcontextprotocol/server-github@latest

# 4) Puppeteer server
npx -y @modelcontextprotocol/server-puppeteer@latest

# 5) Sequential-thinking server
npx -y @modelcontextprotocol/server-sequential-thinking

# 6) Git server (uses uvx wrapper as in .mcp.json; if uvx is unavailable, install/replace accordingly)
uvx mcp-server-git

Notes and troubleshooting for auth/permission errors
- "Unauthorized" or 401 errors for OpenAI: verify OPENAI_API_KEY is set and not expired; check OPENAI_TIMEOUT in `.env` if requests time out.
- "Forbidden" or 403 from GitHub MCP server: ensure GITHUB_TOKEN has the correct scopes (repo/read or repo for private repo access) and is exported to the terminal where you start the server.
- If an MCP server requires an API key and you get permission errors, set MCP_API_KEY in your `.env` and export it in the shell used to start both the MCP server and the Python application.
- Filesystem MCP returns path errors if the provided path is not accessible or is not absolute. Use absolute paths (Windows example shown in `.mcp.json`).
- If MCP discovery fails at startup, check that `settings.MCP_CONFIG.MCP_CONFIG_PATH` points to the correct `.mcp.json` and that `MCP_ENABLED` is set to true in `.env` (if you disabled MCP via env, nothing will be registered).

Quick workflow to avoid auth errors
1. Create and populate `.env` with OPENAI_API_KEY, GITHUB_TOKEN (if needed), and MCP_API_KEY (if your servers require it).
2. Start required MCP servers (see commands above) in separate terminals, ensuring relevant env vars are present in those terminals.
3. Start the Python application:
```
python src/main_orchestrator.py
```

If you still see authentication errors, copy the exact error text and I will analyze it and propose the precise fix.

---

## ğŸ§ª Running Tests

This project includes both unit and integration tests. Some tests require external services (MCP servers, log server) to be running. Follow these steps to run tests reliably.

1) Quick unit tests (no external MCP servers required):
```bash
# From project root
pytest tests/unit -q
```

2) Integration tests (MCP servers required):
- Start the MCP servers defined in `.mcp.json` (see Authentication & MCP Server Startup section). Launch each server in its own terminal so they stay running.
- Ensure any required tokens (GITHUB_TOKEN, MCP_API_KEY, OPENAI_API_KEY) are exported in the terminals where you start the MCP servers and in the terminal where you run tests.

Example:
```bash
# Start filesystem server (example)
npx -y @modelcontextprotocol/server-filesystem@latest "C:\\Users\\pirat\\PycharmProjects\\AI_llm"
# Start other servers as needed (memory, github, puppeteer, sequential-thinking)

# Then run integration tests
pytest tests/integration -q
```

3) Error-handling / socket-based tests:
- Some tests expect a log server (socket) to be available. Start the log server before running these tests:
```bash
python src/utils/error_transfer.py  # or python src/utils/error_transfer.py --help
```

4) Alternative: run the custom runner
```bash
python tests/run_tests.py
```
This runner provides an interactive menu and guidance if some services are missing.

Notes:
- If an integration test fails with authentication/permission errors, confirm the relevant environment variables are set and that the MCP server was started from the terminal that has the environment variables exported.
- For CI, make sure to mock or provision MCP servers and secrets appropriately.

---

## ğŸ’¬ **Usage Guide**

### **Basic Conversation**
```
You: What is the capital of France?
AI: The capital of France is Paris...
```

### **Tool Commands**
```bash
# Force web search
/search latest AI developments

# Force LLM response
/use ai explain quantum computing

# Force tool selection
/use tool translate "hello" to Spanish

# Shell command execution
/shell dir
/shell python --version
```

### **Agent Mode**
```bash
# Trigger intelligent agent orchestration
/agent search for Python tutorials and save the best ones to a file

# Agent will automatically:
# 1. Use GoogleSearch to find Python tutorials
# 2. Evaluate and filter results
# 3. Use filesystem tools to save content
# 4. Provide comprehensive summary
```

### **Application Control**
```bash
exit           # Graceful shutdown with cleanup
Ctrl+C         # Emergency exit
```

---

## ğŸ—ï¸ **Project Architecture**

The AI-Agent-Workflow Project follows a modular, enterprise-grade architecture with clear separation of concerns, designed for scalability and maintainability:

### **ğŸ¯ Core System Components**
```
ğŸ“ src/
â”œâ”€â”€ ğŸš€ main_orchestrator.py                    # Application entry point with Rich Traceback
â”œâ”€â”€ ğŸ“ agents/                                 # Multi-agent orchestration system
â”‚   â”œâ”€â”€ ğŸ¤– agent_mode_node.py                 # Agent mode orchestration with context tracking
â”‚   â”œâ”€â”€ ğŸ’¬ chat_llm.py                        # LLM communication and response handling
â”‚   â”œâ”€â”€ ğŸ” classify_agent.py                  # Message classification for /agent detection
â”‚   â”œâ”€â”€ ğŸ§­ router.py                          # Message routing between processing nodes
â”‚   â””â”€â”€ ğŸ› ï¸ tool_selector.py                   # Tool selection logic based on user input
â”œâ”€â”€ ğŸ“ config/                                # Configuration management
â”‚   â”œâ”€â”€ âš™ï¸ settings.py                        # Application settings and configuration variables
â”‚   â””â”€â”€ ğŸ“ configure_logging.py               # Logging configuration and setup helpers
â”œâ”€â”€ ğŸ“ core/                                  # Core system components
â”‚   â”œâ”€â”€ ğŸ¬ chat_initializer.py                # Chat system initialization and setup
â”‚   â””â”€â”€ ğŸ“ graphs/                            # LangGraph workflow definitions
â”œâ”€â”€ ğŸ“ models/                                # Data models and state management
â”‚   â””â”€â”€ ğŸ”„ state.py                           # State management with StateAccessor singleton
â”œâ”€â”€ ğŸ“ prompts/                               # AI prompt templates
â”‚   â”œâ”€â”€ ğŸ¯ agent_mode_prompts.py              # Prompts for agent mode operations
â”‚   â””â”€â”€ ğŸ’­ open_ai_prompt.py                  # OpenAI-specific prompt templates
â””â”€â”€ ğŸ“ utils/                                 # Utility modules and services
    â”œâ”€â”€ ğŸ”€ model_manager.py                   # Hybrid model management (Ollama/OpenAI)
    â”œâ”€â”€ ğŸŒ open_ai_integration.py             # OpenAI/NVIDIA API integration with circuit breaker
    â””â”€â”€ ğŸ“ listeners/                         # Event-driven architecture
        â”œâ”€â”€ ğŸ“¡ event_listener.py              # Core event management system
        â””â”€â”€ ğŸ¨ rich_status_listen.py          # Rich status integration
```

### **ğŸ› ï¸ Advanced Tools Ecosystem**
```
ğŸ“ src/tools/lggraph_tools/
â”œâ”€â”€ ğŸ“‹ tool_assign.py                         # Tool registry and assignment management
â”œâ”€â”€ ğŸ“¤ tool_response_manager.py               # Response handling from tool executions
â”œâ”€â”€ ğŸ“ tools/                                # Core tool implementations
â”‚   â”œâ”€â”€ ğŸ” google_search_tool.py             # Google search functionality
â”‚   â”œâ”€â”€ ğŸ§  rag_search_classifier_tool.py     # Knowledge base search / RAG (classifier variant)
â”‚   â”œâ”€â”€ ğŸŒ translate_tool.py                 # Translation services
â”‚   â”œâ”€â”€ ğŸ’» run_shell_command_tool.py         # Shell command execution
â”‚   â””â”€â”€ ğŸ“ mcp_integrated_tools/             # MCP filesystem integration
â”‚       â”œâ”€â”€ ğŸ“‚ filesystem.py                 # File operations (exposes 14 MCP filesystem actions)
â”‚       â””â”€â”€ ğŸ“‚ universal.py                  # Universal MCP adapter for dynamic tool routing
â””â”€â”€ ğŸ“ tool_schemas/                          # Tool argument schemas and validation
```

The MCP filesystem server exposes the following dynamic operations (14 actions) when registered and running:
- read_file
- read_text_file
- read_media_file
- read_multiple_files
- write_file
- edit_file
- create_directory
- list_directory
- list_directory_with_sizes
- directory_tree
- move_file
- search_files
- get_file_info
- list_allowed_directories

These operation names are the ones used by the Universal MCP adapter (src/tools/lggraph_tools/tools/mcp_integrated_tools/universal.py) to map tool calls to the filesystem server.

### **ğŸ¨ Modern UI & Diagnostics**
```
ğŸ“ src/ui/
â”œâ”€â”€ ğŸ¨ print_message_style.py                # Message formatting and styling
â”œâ”€â”€ ğŸª print_banner.py                       # Application banner display
â””â”€â”€ ğŸ“ diagnostics/                          # Rich Traceback system
    â”œâ”€â”€ ğŸ”§ rich_traceback_manager.py         # Enterprise-grade error handling
    â”œâ”€â”€ ğŸ›Ÿ debug_helpers.py                  # Debug message helpers
    â””â”€â”€ ğŸ“¨ debug_message_protocol.py         # Debug transport protocol
```

### **ğŸ”Œ Enhanced MCP Integration**
```
ğŸ“ src/mcp/
â”œâ”€â”€ ğŸ›ï¸ manager.py                            # MCP server lifecycle management
â”œâ”€â”€ ğŸ”„ dynamically_tool_register.py          # Dynamic MCP tool registration
â”œâ”€â”€ ğŸ“¥ load_config.py                        # MCP configuration loading (.mcp.json)
â””â”€â”€ ğŸ—ï¸ mcp_register_structure.py            # MCP registration structure definitions
```

### **ğŸ§  Next-Gen RAG System**
```
ğŸ“ src/RAG/
â””â”€â”€ ğŸ“ RAG_FILES/                            # Knowledge base and retrieval files
    â”œâ”€â”€ ğŸ—„ï¸ neo4j_rag.py                      # Neo4j graph database integration
    â””â”€â”€ ğŸ“š knowledge_base/                   # Document storage and indexing
```

### **ğŸ§ª Testing Infrastructure**
```
ğŸ“ tests/
â”œâ”€â”€ ğŸ”¬ run_tests.py                          # Test suite execution
â”œâ”€â”€ ğŸ“ event_listener/                       # Event system testing
â”‚   â”œâ”€â”€ ğŸ¯ quick_validation.py              # Fast event system validation
â”‚   â”œâ”€â”€ ğŸ§ª test_event_listener_realistic.py # Realistic event testing scenarios
â”‚   â””â”€â”€ ğŸ“Š run_listener_test.py             # Comprehensive listener testing
â””â”€â”€ ğŸ“ integration/                          # Integration testing
    â”œâ”€â”€ ğŸ”— test_mcp_integration.py          # MCP server integration tests
    â””â”€â”€ ğŸ¤– test_agent_mode.py               # Agent mode functionality tests
```

### **ğŸ“Š Configuration & DevOps**
```
ğŸ“ Project Root
â”œâ”€â”€ ğŸ³ Dockerfile                           # Container deployment configuration
â”œâ”€â”€ ğŸ™ docker-compose.yml                   # Multi-container orchestration
â”œâ”€â”€ âš™ï¸ .mcp.json                            # Dynamic MCP server configuration
â”œâ”€â”€ ğŸ”§ pyproject.toml                       # Python project configuration
â”œâ”€â”€ ğŸ“¦ requirements.txt                     # Python dependencies
â”œâ”€â”€ ğŸŒ .env                                 # Environment variables
â””â”€â”€ ğŸ“ copilot_instructions/                # Development guidelines
    â””â”€â”€ ğŸ“˜ mcp_instructions.md              # MCP integration guidelines
```

---

### Directory Purpose Documentation

**`src/utils/`** - Supporting infrastructure utilities
- `argument_schema_util.py` - Tool argument schema extraction and validation
- `error_transfer.py` - Raw socket server for debug messages and error logs
- `model_manager.py` - Local/OpenAI model multiplexing with hybrid switching
- `open_ai_integration.py` - NVIDIA-compatible OpenAI adapter with singleton pattern
- `socket_manager.py` - Subprocess log server management with legacy bridge
- `listeners/` - Event-driven architecture with Rich status integration

**`src/ui/diagnostics/`** - Structured logging and Rich traceback management
- `rich_traceback_manager.py` - Enterprise-grade error handling system
- `debug_helpers.py` - Structured debug utilities and message routing
- `debug_message_protocol.py` - Debug message protocol implementation

**`src/agents/`** - Multi-agent orchestration layer
- `agent_mode_node.py` - Complete agent mode implementation with tool orchestration
- `classify_agent.py` - Message classification and routing logic
- `chat_llm.py` - LLM communication and response handling
- `router.py` - Message routing between processing nodes
- `tool_selector.py` - Tool selection logic based on user input

**`src/tools/lggraph_tools/`** - 17-tool ecosystem
- 3 fundamental tools: GoogleSearch, RAGSearch, Translate
- 14 dynamic MCP filesystem tools
- Tool selection and execution logic
- Response management and validation

**`src/mcp/`** - Model Context Protocol implementation
- JSON-RPC communication with subprocess management
- Dynamic tool discovery and registration
- Server lifecycle management
- Configuration loading and validation

---

## ğŸ¤– Agent Mode

Advanced multi-tool orchestration system with AI-powered parameter generation.

### Features
- **AI-Powered Parameter Generation** - Intelligent parameter creation for tool execution
- **Sequential Tool Processing** - Coordinated execution of multiple tools
- **Failure Recovery** - Automatic retry and error handling
- **Context Awareness** - Maintains context across tool executions
- **Final Response Evaluation** - Quality assessment and optimization (v4.0 simplified)

### Usage
Agent mode is automatically activated for complex multi-step tasks that require tool orchestration. Use `/agent` command to explicitly trigger agent mode.

### Example Workflow
```
/agent search for Python tutorials and save the best ones to a file

Agent will automatically:
1. Use GoogleSearch to find Python tutorials
2. Evaluate and filter results
3. Use filesystem tools to save content
4. Provide comprehensive summary
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### MIT License Summary
- **Commercial Use** - Permitted
- **Modification** - Permitted
- **Distribution** - Permitted
- **Private Use** - Permitted
- **Liability** - Limited
- **Warranty** - None provided

---

## ğŸ¯ Roadmap

### Near-term Goals (Next 3 months)
- **Complete Agent Mode Optimization** - Achieve 99% reliability
- **Enhanced Tool Ecosystem** - Add 10+ additional MCP tools
- **Performance Improvements** - 50% faster response times
- **Extended MCP Server Support** - Support for custom MCP implementations

### Medium-term Vision (6-12 months)
- **Production Deployment Capabilities** - Container orchestration and CI/CD
- **Advanced AI Agent Orchestration** - Multi-agent collaboration patterns
- **Enterprise Integration Features** - SSO, audit logging, compliance features
- **Comprehensive Developer Tools** - IDE extensions, debugging tools, profilers

### Long-term Vision (1+ years)
- **Distributed Agent Networks** - Multi-node agent coordination
- **Advanced Reasoning Capabilities** - Enhanced planning and execution
- **Industry-Specific Solutions** - Specialized agent configurations
- **Open-Source Ecosystem** - Community-driven tool and server development

---

**Built with â¤ï¸ for enterprise-grade AI agent development**

*AI-Agent-Workflow Project v1.8.0 - Transforming AI assistant development with enterprise-grade architecture and professional workflows.*
